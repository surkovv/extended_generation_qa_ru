nohup: ignoring input
2022-10-07 12:33:39.402 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from https://files.deeppavlov.ai/deeppavlov_data/extended_generative_qa/msmarco/ms_marco_preprocessed.json to /data/home/admin/.deeppavlov/downloads/ms_marco_preprocessed.json
  0%|          | 0.00/136M [00:00<?, ?B/s] 14%|█▍        | 19.1M/136M [00:00<00:00, 191MB/s] 32%|███▏      | 43.6M/136M [00:00<00:00, 223MB/s] 50%|█████     | 68.3M/136M [00:00<00:00, 234MB/s] 68%|██████▊   | 92.6M/136M [00:00<00:00, 237MB/s] 86%|████████▋ | 117M/136M [00:00<00:00, 240MB/s] 100%|██████████| 136M/136M [00:00<00:00, 236MB/s]
[nltk_data] Downloading package punkt to /home/admin/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to /home/admin/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[nltk_data] Downloading package perluniprops to
[nltk_data]     /home/admin/nltk_data...
[nltk_data]   Package perluniprops is already up-to-date!
[nltk_data] Downloading package nonbreaking_prefixes to
[nltk_data]     /home/admin/nltk_data...
[nltk_data]   Package nonbreaking_prefixes is already up-to-date!
Downloading:   0%|          | 0.00/82.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 82.0/82.0 [00:00<00:00, 118kB/s]
Downloading:   0%|          | 0.00/553 [00:00<?, ?B/s]Downloading: 100%|██████████| 553/553 [00:00<00:00, 820kB/s]
Downloading:   0%|          | 0.00/4.11M [00:00<?, ?B/s]Downloading:   0%|          | 4.00k/4.11M [00:00<02:11, 32.8kB/s]Downloading:   1%|          | 24.0k/4.11M [00:00<00:39, 110kB/s] Downloading:   1%|          | 40.0k/4.11M [00:00<00:35, 119kB/s]Downloading:   1%|          | 52.0k/4.11M [00:00<00:38, 111kB/s]Downloading:   2%|▏         | 69.0k/4.11M [00:00<00:35, 121kB/s]Downloading:   2%|▏         | 85.0k/4.11M [00:00<00:34, 124kB/s]Downloading:   3%|▎         | 109k/4.11M [00:00<00:28, 148kB/s] Downloading:   3%|▎         | 124k/4.11M [00:01<00:29, 139kB/s]Downloading:   3%|▎         | 141k/4.11M [00:01<00:29, 139kB/s]Downloading:   4%|▎         | 157k/4.11M [00:01<00:30, 137kB/s]Downloading:   4%|▍         | 189k/4.11M [00:01<00:23, 174kB/s]Downloading:   5%|▍         | 206k/4.11M [00:01<00:25, 164kB/s]Downloading:   6%|▌         | 237k/4.11M [00:01<00:21, 190kB/s]Downloading:   6%|▌         | 256k/4.11M [00:01<00:22, 180kB/s]Downloading:   7%|▋         | 285k/4.11M [00:01<00:20, 197kB/s]Downloading:   8%|▊         | 317k/4.11M [00:02<00:18, 216kB/s]Downloading:   8%|▊         | 339k/4.11M [00:02<00:19, 205kB/s]Downloading:   9%|▊         | 365k/4.11M [00:02<00:18, 208kB/s]Downloading:   9%|▉         | 397k/4.11M [00:02<00:17, 224kB/s]Downloading:  11%|█         | 445k/4.11M [00:02<00:14, 273kB/s]Downloading:  11%|█▏        | 477k/4.11M [00:02<00:14, 270kB/s]Downloading:  12%|█▏        | 509k/4.11M [00:02<00:14, 268kB/s]Downloading:  13%|█▎        | 557k/4.11M [00:02<00:12, 304kB/s]Downloading:  14%|█▍        | 589k/4.11M [00:03<00:12, 292kB/s]Downloading:  15%|█▌        | 637k/4.11M [00:03<00:11, 322kB/s]Downloading:  16%|█▋        | 685k/4.11M [00:03<00:10, 342kB/s]Downloading:  17%|█▋        | 733k/4.11M [00:03<00:09, 357kB/s]Downloading:  19%|█▊        | 781k/4.11M [00:03<00:09, 368kB/s]Downloading:  20%|█▉        | 829k/4.11M [00:03<00:09, 375kB/s]Downloading:  21%|██        | 893k/4.11M [00:03<00:08, 419kB/s]Downloading:  22%|██▏       | 941k/4.11M [00:03<00:08, 411kB/s]Downloading:  24%|██▍       | 0.98M/4.11M [00:04<00:07, 444kB/s]Downloading:  25%|██▌       | 1.04M/4.11M [00:04<00:06, 468kB/s]Downloading:  27%|██▋       | 1.11M/4.11M [00:04<00:06, 484kB/s]Downloading:  29%|██▉       | 1.18M/4.11M [00:04<00:05, 534kB/s]Downloading:  30%|███       | 1.25M/4.11M [00:04<00:05, 531kB/s]Downloading:  32%|███▏      | 1.33M/4.11M [00:04<00:05, 567kB/s]Downloading:  34%|███▍      | 1.40M/4.11M [00:04<00:04, 593kB/s]Downloading:  36%|███▌      | 1.48M/4.11M [00:04<00:04, 611kB/s]Downloading:  38%|███▊      | 1.58M/4.11M [00:05<00:04, 662kB/s]Downloading:  41%|████      | 1.67M/4.11M [00:05<00:03, 698kB/s]Downloading:  43%|████▎     | 1.76M/4.11M [00:05<00:03, 724kB/s]Downloading:  45%|████▌     | 1.86M/4.11M [00:05<00:03, 742kB/s]Downloading:  48%|████▊     | 1.97M/4.11M [00:05<00:02, 793kB/s]Downloading:  50%|█████     | 2.08M/4.11M [00:05<00:02, 829kB/s]Downloading:  54%|█████▎    | 2.20M/4.11M [00:05<00:02, 893kB/s]Downloading:  56%|█████▌    | 2.31M/4.11M [00:05<00:02, 900kB/s]Downloading:  60%|█████▉    | 2.45M/4.11M [00:06<00:01, 981kB/s]Downloading:  63%|██████▎   | 2.58M/4.11M [00:06<00:01, 1.00MB/s]Downloading:  66%|██████▌   | 2.72M/4.11M [00:06<00:01, 1.05MB/s]Downloading:  70%|██████▉   | 2.87M/4.11M [00:06<00:01, 1.13MB/s]Downloading:  74%|███████▎  | 3.03M/4.11M [00:06<00:00, 1.18MB/s]Downloading:  77%|███████▋  | 3.18M/4.11M [00:06<00:00, 1.22MB/s]Downloading:  82%|████████▏ | 3.36M/4.11M [00:06<00:00, 1.28MB/s]Downloading:  86%|████████▌ | 3.54M/4.11M [00:06<00:00, 1.37MB/s]Downloading:  91%|█████████ | 3.73M/4.11M [00:07<00:00, 1.43MB/s]Downloading:  96%|█████████▌| 3.93M/4.11M [00:07<00:00, 1.51MB/s]Downloading: 100%|██████████| 4.11M/4.11M [00:07<00:00, 602kB/s] 
Downloading:   0%|          | 0.00/99.0 [00:00<?, ?B/s]Downloading: 100%|██████████| 99.0/99.0 [00:00<00:00, 120kB/s]
2022-10-07 12:34:10.259 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 145: From pretrained google/mt5-small.
Downloading:   0%|          | 0.00/1.12G [00:00<?, ?B/s]Downloading:   0%|          | 712k/1.12G [00:00<02:44, 7.29MB/s]Downloading:   0%|          | 2.69M/1.12G [00:00<01:33, 12.9MB/s]Downloading:   0%|          | 3.87M/1.12G [00:00<01:35, 12.5MB/s]Downloading:   1%|          | 6.82M/1.12G [00:00<01:02, 19.1MB/s]Downloading:   1%|          | 8.65M/1.12G [00:00<01:04, 18.4MB/s]Downloading:   1%|          | 10.4M/1.12G [00:00<01:05, 18.3MB/s]Downloading:   1%|          | 12.2M/1.12G [00:00<01:05, 18.3MB/s]Downloading:   1%|          | 13.9M/1.12G [00:00<01:06, 17.9MB/s]Downloading:   1%|▏         | 15.6M/1.12G [00:00<01:06, 17.8MB/s]Downloading:   2%|▏         | 17.3M/1.12G [00:01<01:06, 17.8MB/s]Downloading:   2%|▏         | 19.0M/1.12G [00:01<01:06, 17.8MB/s]Downloading:   2%|▏         | 20.8M/1.12G [00:01<01:05, 17.9MB/s]Downloading:   2%|▏         | 22.5M/1.12G [00:01<01:06, 17.7MB/s]Downloading:   2%|▏         | 24.2M/1.12G [00:01<01:06, 17.7MB/s]Downloading:   2%|▏         | 26.0M/1.12G [00:01<01:05, 17.8MB/s]Downloading:   2%|▏         | 27.7M/1.12G [00:01<01:05, 17.9MB/s]Downloading:   3%|▎         | 29.6M/1.12G [00:01<01:06, 17.7MB/s]Downloading:   3%|▎         | 31.4M/1.12G [00:01<01:04, 18.0MB/s]Downloading:   3%|▎         | 33.1M/1.12G [00:01<01:05, 17.7MB/s]Downloading:   3%|▎         | 34.8M/1.12G [00:02<01:05, 17.7MB/s]Downloading:   3%|▎         | 36.5M/1.12G [00:02<01:05, 17.8MB/s]Downloading:   3%|▎         | 38.3M/1.12G [00:02<01:03, 18.2MB/s]Downloading:   3%|▎         | 40.0M/1.12G [00:02<01:05, 17.7MB/s]Downloading:   4%|▎         | 41.7M/1.12G [00:02<01:05, 17.7MB/s]Downloading:   4%|▍         | 43.4M/1.12G [00:02<01:05, 17.7MB/s]Downloading:   4%|▍         | 45.2M/1.12G [00:02<01:05, 17.6MB/s]Downloading:   4%|▍         | 47.0M/1.12G [00:02<01:04, 17.9MB/s]Downloading:   4%|▍         | 48.7M/1.12G [00:02<01:05, 17.6MB/s]Downloading:   4%|▍         | 50.4M/1.12G [00:03<01:04, 17.7MB/s]Downloading:   5%|▍         | 52.1M/1.12G [00:03<01:04, 17.7MB/s]Downloading:   5%|▍         | 54.1M/1.12G [00:03<01:01, 18.7MB/s]Downloading:   5%|▍         | 55.9M/1.12G [00:03<01:04, 17.7MB/s]Downloading:   5%|▌         | 57.6M/1.12G [00:03<01:05, 17.4MB/s]Downloading:   5%|▌         | 59.3M/1.12G [00:03<01:06, 17.2MB/s]Downloading:   5%|▌         | 61.1M/1.12G [00:03<01:04, 17.6MB/s]Downloading:   6%|▌         | 63.1M/1.12G [00:03<01:03, 17.7MB/s]Downloading:   6%|▌         | 64.9M/1.12G [00:03<01:03, 18.0MB/s]Downloading:   6%|▌         | 66.6M/1.12G [00:03<01:04, 17.7MB/s]Downloading:   6%|▌         | 68.3M/1.12G [00:04<01:03, 17.7MB/s]Downloading:   6%|▌         | 70.0M/1.12G [00:04<01:03, 17.7MB/s]Downloading:   6%|▋         | 71.9M/1.12G [00:04<01:03, 17.7MB/s]Downloading:   6%|▋         | 73.7M/1.12G [00:04<01:02, 17.9MB/s]Downloading:   7%|▋         | 75.4M/1.12G [00:04<01:03, 17.8MB/s]Downloading:   7%|▋         | 77.1M/1.12G [00:04<01:03, 17.8MB/s]Downloading:   7%|▋         | 78.8M/1.12G [00:04<01:03, 17.7MB/s]Downloading:   7%|▋         | 80.6M/1.12G [00:04<01:01, 18.1MB/s]Downloading:   7%|▋         | 82.4M/1.12G [00:04<01:03, 17.6MB/s]Downloading:   7%|▋         | 84.1M/1.12G [00:05<01:03, 17.6MB/s]Downloading:   7%|▋         | 85.8M/1.12G [00:05<01:02, 17.7MB/s]Downloading:   8%|▊         | 87.5M/1.12G [00:05<01:02, 17.7MB/s]Downloading:   8%|▊         | 89.2M/1.12G [00:05<01:02, 17.8MB/s]Downloading:   8%|▊         | 90.9M/1.12G [00:05<01:02, 17.7MB/s]Downloading:   8%|▊         | 92.6M/1.12G [00:05<01:07, 16.5MB/s]Downloading:   8%|▊         | 94.2M/1.12G [00:05<01:08, 16.2MB/s]Downloading:   8%|▊         | 95.7M/1.12G [00:05<01:08, 16.2MB/s]Downloading:   8%|▊         | 97.3M/1.12G [00:05<01:10, 15.5MB/s]Downloading:   9%|▊         | 98.8M/1.12G [00:05<01:12, 15.2MB/s]Downloading:   9%|▉         | 100M/1.12G [00:06<01:13, 15.0MB/s] Downloading:   9%|▉         | 102M/1.12G [00:06<01:14, 14.8MB/s]Downloading:   9%|▉         | 103M/1.12G [00:06<01:14, 14.6MB/s]Downloading:   9%|▉         | 104M/1.12G [00:06<01:14, 14.6MB/s]Downloading:   9%|▉         | 106M/1.12G [00:06<01:15, 14.5MB/s]Downloading:   9%|▉         | 107M/1.12G [00:06<01:15, 14.4MB/s]Downloading:   9%|▉         | 109M/1.12G [00:06<01:13, 14.9MB/s]Downloading:  10%|▉         | 111M/1.12G [00:06<01:07, 16.0MB/s]Downloading:  10%|▉         | 112M/1.12G [00:06<01:08, 15.7MB/s]Downloading:  10%|█         | 115M/1.12G [00:07<00:56, 19.0MB/s]Downloading:  10%|█         | 117M/1.12G [00:07<00:58, 18.5MB/s]Downloading:  10%|█         | 118M/1.12G [00:07<00:59, 18.2MB/s]Downloading:  10%|█         | 120M/1.12G [00:07<00:59, 18.0MB/s]Downloading:  11%|█         | 122M/1.12G [00:07<01:00, 17.8MB/s]Downloading:  11%|█         | 124M/1.12G [00:07<01:00, 17.7MB/s]Downloading:  11%|█         | 125M/1.12G [00:07<01:00, 17.7MB/s]Downloading:  11%|█         | 127M/1.12G [00:07<01:00, 17.6MB/s]Downloading:  11%|█         | 129M/1.12G [00:07<01:00, 17.6MB/s]Downloading:  11%|█▏        | 130M/1.12G [00:07<01:00, 17.6MB/s]Downloading:  12%|█▏        | 132M/1.12G [00:08<01:00, 17.4MB/s]Downloading:  12%|█▏        | 134M/1.12G [00:08<01:01, 17.3MB/s]Downloading:  12%|█▏        | 136M/1.12G [00:08<00:56, 18.6MB/s]Downloading:  12%|█▏        | 138M/1.12G [00:08<00:57, 18.2MB/s]Downloading:  12%|█▏        | 139M/1.12G [00:08<00:58, 18.0MB/s]Downloading:  12%|█▏        | 141M/1.12G [00:08<00:59, 17.8MB/s]Downloading:  12%|█▏        | 143M/1.12G [00:08<00:59, 17.7MB/s]Downloading:  13%|█▎        | 144M/1.12G [00:08<00:59, 17.6MB/s]Downloading:  13%|█▎        | 146M/1.12G [00:08<00:59, 17.6MB/s]Downloading:  13%|█▎        | 148M/1.12G [00:08<00:59, 17.6MB/s]Downloading:  13%|█▎        | 149M/1.12G [00:09<00:59, 17.5MB/s]Downloading:  13%|█▎        | 151M/1.12G [00:09<00:59, 17.5MB/s]Downloading:  13%|█▎        | 153M/1.12G [00:09<00:59, 17.4MB/s]Downloading:  13%|█▎        | 154M/1.12G [00:09<01:01, 16.9MB/s]Downloading:  14%|█▎        | 157M/1.12G [00:09<00:55, 18.7MB/s]Downloading:  14%|█▍        | 158M/1.12G [00:09<00:56, 18.3MB/s]Downloading:  14%|█▍        | 160M/1.12G [00:09<00:57, 18.0MB/s]Downloading:  14%|█▍        | 162M/1.12G [00:09<00:57, 17.9MB/s]Downloading:  14%|█▍        | 164M/1.12G [00:09<00:58, 17.7MB/s]Downloading:  14%|█▍        | 165M/1.12G [00:09<00:58, 17.6MB/s]Downloading:  15%|█▍        | 167M/1.12G [00:10<00:58, 17.5MB/s]Downloading:  15%|█▍        | 169M/1.12G [00:10<01:37, 10.5MB/s]Downloading:  15%|█▍        | 170M/1.12G [00:10<01:25, 11.9MB/s]Downloading:  15%|█▌        | 172M/1.12G [00:10<01:17, 13.1MB/s]Downloading:  15%|█▌        | 174M/1.12G [00:10<01:03, 16.0MB/s]Downloading:  15%|█▌        | 176M/1.12G [00:10<01:05, 15.5MB/s]Downloading:  16%|█▌        | 178M/1.12G [00:10<01:01, 16.4MB/s]Downloading:  16%|█▌        | 180M/1.12G [00:11<00:57, 17.6MB/s]Downloading:  16%|█▌        | 182M/1.12G [00:11<00:57, 17.6MB/s]Downloading:  16%|█▌        | 184M/1.12G [00:11<00:57, 17.6MB/s]Downloading:  16%|█▌        | 185M/1.12G [00:11<00:57, 17.6MB/s]Downloading:  16%|█▋        | 187M/1.12G [00:11<00:57, 17.6MB/s]Downloading:  16%|█▋        | 189M/1.12G [00:11<00:57, 17.6MB/s]Downloading:  17%|█▋        | 190M/1.12G [00:11<00:57, 17.5MB/s]Downloading:  17%|█▋        | 192M/1.12G [00:11<00:57, 17.5MB/s]Downloading:  17%|█▋        | 194M/1.12G [00:11<00:56, 17.5MB/s]Downloading:  17%|█▋        | 195M/1.12G [00:11<00:59, 16.8MB/s]Downloading:  17%|█▋        | 197M/1.12G [00:12<00:59, 16.7MB/s]Downloading:  17%|█▋        | 199M/1.12G [00:12<00:56, 17.7MB/s]Downloading:  18%|█▊        | 201M/1.12G [00:12<00:55, 17.7MB/s]Downloading:  18%|█▊        | 202M/1.12G [00:12<00:55, 17.7MB/s]Downloading:  18%|█▊        | 204M/1.12G [00:12<00:55, 17.7MB/s]Downloading:  18%|█▊        | 206M/1.12G [00:12<00:54, 18.0MB/s]Downloading:  18%|█▊        | 208M/1.12G [00:12<00:55, 17.8MB/s]Downloading:  18%|█▊        | 209M/1.12G [00:12<00:55, 17.7MB/s]Downloading:  18%|█▊        | 211M/1.12G [00:12<00:55, 17.7MB/s]Downloading:  19%|█▊        | 213M/1.12G [00:13<00:55, 17.7MB/s]Downloading:  19%|█▊        | 215M/1.12G [00:13<00:54, 17.9MB/s]Downloading:  19%|█▉        | 216M/1.12G [00:13<00:54, 17.9MB/s]Downloading:  19%|█▉        | 218M/1.12G [00:13<00:55, 17.7MB/s]Downloading:  19%|█▉        | 220M/1.12G [00:13<00:54, 17.8MB/s]Downloading:  19%|█▉        | 222M/1.12G [00:13<00:54, 17.7MB/s]Downloading:  20%|█▉        | 223M/1.12G [00:13<00:53, 17.9MB/s]Downloading:  20%|█▉        | 225M/1.12G [00:13<00:54, 17.7MB/s]Downloading:  20%|█▉        | 227M/1.12G [00:13<00:54, 17.7MB/s]Downloading:  20%|█▉        | 229M/1.12G [00:13<00:53, 17.8MB/s]Downloading:  20%|██        | 230M/1.12G [00:14<00:53, 17.8MB/s]Downloading:  20%|██        | 232M/1.12G [00:14<00:53, 17.8MB/s]Downloading:  20%|██        | 234M/1.12G [00:14<00:53, 17.7MB/s]Downloading:  21%|██        | 236M/1.12G [00:14<00:53, 17.8MB/s]Downloading:  21%|██        | 237M/1.12G [00:14<00:54, 17.6MB/s]Downloading:  21%|██        | 239M/1.12G [00:14<00:52, 17.9MB/s]Downloading:  21%|██        | 241M/1.12G [00:14<00:53, 17.7MB/s]Downloading:  21%|██        | 242M/1.12G [00:14<00:53, 17.6MB/s]Downloading:  21%|██▏       | 244M/1.12G [00:14<00:53, 17.8MB/s]Downloading:  21%|██▏       | 246M/1.12G [00:14<00:53, 17.7MB/s]Downloading:  22%|██▏       | 248M/1.12G [00:15<00:52, 17.8MB/s]Downloading:  22%|██▏       | 249M/1.12G [00:15<00:52, 17.7MB/s]Downloading:  22%|██▏       | 251M/1.12G [00:15<00:52, 17.8MB/s]Downloading:  22%|██▏       | 253M/1.12G [00:15<00:52, 17.7MB/s]Downloading:  22%|██▏       | 255M/1.12G [00:15<00:52, 17.9MB/s]Downloading:  22%|██▏       | 256M/1.12G [00:15<00:52, 17.7MB/s]Downloading:  23%|██▎       | 258M/1.12G [00:15<00:52, 17.8MB/s]Downloading:  23%|██▎       | 260M/1.12G [00:15<00:52, 17.8MB/s]Downloading:  23%|██▎       | 262M/1.12G [00:15<00:50, 18.4MB/s]Downloading:  23%|██▎       | 263M/1.12G [00:15<00:52, 17.5MB/s]Downloading:  23%|██▎       | 265M/1.12G [00:16<00:52, 17.5MB/s]Downloading:  23%|██▎       | 267M/1.12G [00:16<00:53, 17.3MB/s]Downloading:  23%|██▎       | 268M/1.12G [00:16<00:53, 17.2MB/s]Downloading:  24%|██▎       | 270M/1.12G [00:16<00:50, 18.1MB/s]Downloading:  24%|██▍       | 272M/1.12G [00:16<00:50, 18.1MB/s]Downloading:  24%|██▍       | 274M/1.12G [00:16<00:49, 18.6MB/s]Downloading:  24%|██▍       | 276M/1.12G [00:16<00:49, 18.3MB/s]Downloading:  24%|██▍       | 278M/1.12G [00:16<00:50, 18.1MB/s]Downloading:  24%|██▍       | 279M/1.12G [00:16<00:50, 17.8MB/s]Downloading:  25%|██▍       | 281M/1.12G [00:17<00:51, 17.7MB/s]Downloading:  25%|██▍       | 283M/1.12G [00:17<00:51, 17.7MB/s]Downloading:  25%|██▍       | 284M/1.12G [00:17<00:51, 17.6MB/s]Downloading:  25%|██▍       | 286M/1.12G [00:17<00:51, 17.6MB/s]Downloading:  25%|██▌       | 288M/1.12G [00:17<00:51, 17.6MB/s]Downloading:  25%|██▌       | 289M/1.12G [00:17<00:51, 17.6MB/s]Downloading:  25%|██▌       | 291M/1.12G [00:17<00:52, 17.1MB/s]Downloading:  26%|██▌       | 293M/1.12G [00:17<00:50, 17.7MB/s]Downloading:  26%|██▌       | 295M/1.12G [00:17<00:48, 18.5MB/s]Downloading:  26%|██▌       | 297M/1.12G [00:17<00:48, 18.2MB/s]Downloading:  26%|██▌       | 298M/1.12G [00:18<00:49, 18.0MB/s]Downloading:  26%|██▌       | 300M/1.12G [00:18<00:49, 17.9MB/s]Downloading:  26%|██▋       | 302M/1.12G [00:18<00:49, 17.8MB/s]Downloading:  26%|██▋       | 303M/1.12G [00:18<00:49, 17.7MB/s]Downloading:  27%|██▋       | 305M/1.12G [00:18<00:49, 17.6MB/s]Downloading:  27%|██▋       | 307M/1.12G [00:18<00:49, 17.6MB/s]Downloading:  27%|██▋       | 309M/1.12G [00:18<00:50, 17.5MB/s]Downloading:  27%|██▋       | 310M/1.12G [00:18<00:49, 17.6MB/s]Downloading:  27%|██▋       | 312M/1.12G [00:18<00:50, 17.2MB/s]Downloading:  27%|██▋       | 314M/1.12G [00:18<00:48, 17.8MB/s]Downloading:  28%|██▊       | 316M/1.12G [00:19<00:47, 18.4MB/s]Downloading:  28%|██▊       | 318M/1.12G [00:19<00:47, 18.1MB/s]Downloading:  28%|██▊       | 319M/1.12G [00:19<00:48, 18.0MB/s]Downloading:  28%|██▊       | 321M/1.12G [00:19<00:48, 17.8MB/s]Downloading:  28%|██▊       | 323M/1.12G [00:19<00:48, 17.7MB/s]Downloading:  28%|██▊       | 324M/1.12G [00:19<00:48, 17.7MB/s]Downloading:  28%|██▊       | 326M/1.12G [00:19<00:48, 17.6MB/s]Downloading:  29%|██▊       | 328M/1.12G [00:19<00:48, 17.6MB/s]Downloading:  29%|██▉       | 329M/1.12G [00:19<00:48, 17.5MB/s]Downloading:  29%|██▉       | 331M/1.12G [00:19<00:48, 17.6MB/s]Downloading:  29%|██▉       | 333M/1.12G [00:20<00:50, 16.7MB/s]Downloading:  29%|██▉       | 335M/1.12G [00:20<00:50, 17.0MB/s]Downloading:  29%|██▉       | 337M/1.12G [00:20<00:45, 18.7MB/s]Downloading:  30%|██▉       | 339M/1.12G [00:20<00:45, 18.4MB/s]Downloading:  30%|██▉       | 341M/1.12G [00:20<00:46, 18.1MB/s]Downloading:  30%|██▉       | 342M/1.12G [00:20<00:46, 17.9MB/s]Downloading:  30%|███       | 344M/1.12G [00:20<00:47, 17.8MB/s]Downloading:  30%|███       | 346M/1.12G [00:21<01:17, 10.8MB/s]Downloading:  30%|███       | 347M/1.12G [00:21<01:08, 12.2MB/s]Downloading:  30%|███       | 349M/1.12G [00:21<01:02, 13.4MB/s]Downloading:  31%|███       | 351M/1.12G [00:21<00:57, 14.4MB/s]Downloading:  31%|███       | 353M/1.12G [00:21<00:53, 15.7MB/s]Downloading:  31%|███       | 354M/1.12G [00:21<00:53, 15.5MB/s]Downloading:  31%|███       | 356M/1.12G [00:21<00:52, 15.9MB/s]Downloading:  31%|███▏      | 358M/1.12G [00:21<00:43, 19.0MB/s]Downloading:  31%|███▏      | 360M/1.12G [00:21<00:44, 18.6MB/s]Downloading:  32%|███▏      | 362M/1.12G [00:22<00:44, 18.3MB/s]Downloading:  32%|███▏      | 364M/1.12G [00:22<00:45, 18.2MB/s]Downloading:  32%|███▏      | 366M/1.12G [00:22<00:45, 18.0MB/s]Downloading:  32%|███▏      | 367M/1.12G [00:22<00:45, 17.9MB/s]Downloading:  32%|███▏      | 369M/1.12G [00:22<00:45, 17.8MB/s]Downloading:  32%|███▏      | 371M/1.12G [00:22<00:45, 17.8MB/s]Downloading:  33%|███▎      | 373M/1.12G [00:22<00:45, 17.8MB/s]Downloading:  33%|███▎      | 374M/1.12G [00:22<00:45, 17.7MB/s]Downloading:  33%|███▎      | 376M/1.12G [00:22<00:48, 16.6MB/s]Downloading:  33%|███▎      | 378M/1.12G [00:22<00:45, 17.7MB/s]Downloading:  33%|███▎      | 380M/1.12G [00:23<00:43, 18.4MB/s]Downloading:  33%|███▎      | 382M/1.12G [00:23<00:44, 18.2MB/s]Downloading:  33%|███▎      | 383M/1.12G [00:23<00:44, 18.0MB/s]Downloading:  34%|███▎      | 385M/1.12G [00:23<00:44, 17.9MB/s]Downloading:  34%|███▍      | 387M/1.12G [00:23<00:44, 17.8MB/s]Downloading:  34%|███▍      | 389M/1.12G [00:23<00:44, 17.7MB/s]Downloading:  34%|███▍      | 390M/1.12G [00:23<00:44, 17.7MB/s]Downloading:  34%|███▍      | 392M/1.12G [00:23<00:44, 17.7MB/s]Downloading:  34%|███▍      | 394M/1.12G [00:23<00:44, 17.7MB/s]Downloading:  35%|███▍      | 395M/1.12G [00:23<00:44, 17.8MB/s]Downloading:  35%|███▍      | 397M/1.12G [00:24<00:45, 17.3MB/s]Downloading:  35%|███▍      | 399M/1.12G [00:24<00:45, 17.2MB/s]Downloading:  35%|███▍      | 401M/1.12G [00:24<00:42, 18.3MB/s]Downloading:  35%|███▌      | 402M/1.12G [00:24<00:43, 18.1MB/s]Downloading:  35%|███▌      | 404M/1.12G [00:24<00:43, 18.0MB/s]Downloading:  35%|███▌      | 406M/1.12G [00:24<00:43, 17.8MB/s]Downloading:  36%|███▌      | 408M/1.12G [00:24<00:43, 17.8MB/s]Downloading:  36%|███▌      | 409M/1.12G [00:24<00:43, 17.7MB/s]Downloading:  36%|███▌      | 411M/1.12G [00:24<00:43, 17.6MB/s]Downloading:  36%|███▌      | 413M/1.12G [00:25<00:47, 16.3MB/s]Downloading:  36%|███▌      | 414M/1.12G [00:25<00:47, 16.3MB/s]Downloading:  36%|███▋      | 416M/1.12G [00:25<00:46, 16.3MB/s]Downloading:  36%|███▋      | 417M/1.12G [00:25<00:47, 16.2MB/s]Downloading:  37%|███▋      | 419M/1.12G [00:25<00:46, 16.3MB/s]Downloading:  37%|███▋      | 421M/1.12G [00:25<00:43, 17.4MB/s]Downloading:  37%|███▋      | 423M/1.12G [00:25<00:44, 16.9MB/s]Downloading:  37%|███▋      | 424M/1.12G [00:25<00:44, 16.8MB/s]Downloading:  37%|███▋      | 426M/1.12G [00:25<00:44, 17.0MB/s]Downloading:  37%|███▋      | 428M/1.12G [00:25<00:44, 16.9MB/s]Downloading:  38%|███▊      | 429M/1.12G [00:26<00:42, 17.6MB/s]Downloading:  38%|███▊      | 431M/1.12G [00:26<00:44, 16.9MB/s]Downloading:  38%|███▊      | 433M/1.12G [00:26<00:43, 17.2MB/s]Downloading:  38%|███▊      | 435M/1.12G [00:26<00:42, 17.6MB/s]Downloading:  38%|███▊      | 436M/1.12G [00:26<00:43, 17.1MB/s]Downloading:  38%|███▊      | 438M/1.12G [00:26<00:42, 17.5MB/s]Downloading:  38%|███▊      | 440M/1.12G [00:26<00:42, 17.5MB/s]Downloading:  39%|███▊      | 441M/1.12G [00:26<00:42, 17.4MB/s]Downloading:  39%|███▊      | 443M/1.12G [00:26<00:42, 17.5MB/s]Downloading:  39%|███▉      | 445M/1.12G [00:27<00:41, 17.6MB/s]Downloading:  39%|███▉      | 447M/1.12G [00:27<00:42, 17.4MB/s]Downloading:  39%|███▉      | 448M/1.12G [00:27<00:42, 17.2MB/s]Downloading:  39%|███▉      | 450M/1.12G [00:27<00:39, 18.4MB/s]Downloading:  39%|███▉      | 452M/1.12G [00:27<00:42, 17.2MB/s]Downloading:  40%|███▉      | 454M/1.12G [00:27<00:42, 16.9MB/s]Downloading:  40%|███▉      | 456M/1.12G [00:27<00:42, 17.0MB/s]Downloading:  40%|███▉      | 457M/1.12G [00:27<00:42, 16.9MB/s]Downloading:  40%|████      | 459M/1.12G [00:27<00:39, 18.1MB/s]Downloading:  40%|████      | 461M/1.12G [00:27<00:39, 18.0MB/s]Downloading:  40%|████      | 463M/1.12G [00:28<00:44, 16.1MB/s]Downloading:  41%|████      | 465M/1.12G [00:28<00:41, 17.2MB/s]Downloading:  41%|████      | 466M/1.12G [00:28<00:41, 17.3MB/s]Downloading:  41%|████      | 468M/1.12G [00:28<00:41, 17.2MB/s]Downloading:  41%|████      | 470M/1.12G [00:28<00:38, 18.2MB/s]Downloading:  41%|████      | 472M/1.12G [00:28<00:39, 17.8MB/s]Downloading:  41%|████▏     | 473M/1.12G [00:28<00:41, 17.2MB/s]Downloading:  41%|████▏     | 475M/1.12G [00:28<00:40, 17.4MB/s]Downloading:  42%|████▏     | 477M/1.12G [00:28<00:40, 17.3MB/s]Downloading:  42%|████▏     | 479M/1.12G [00:29<00:39, 17.6MB/s]Downloading:  42%|████▏     | 481M/1.12G [00:29<00:39, 17.6MB/s]Downloading:  42%|████▏     | 483M/1.12G [00:29<00:39, 17.8MB/s]Downloading:  42%|████▏     | 484M/1.12G [00:29<00:39, 17.6MB/s]Downloading:  42%|████▏     | 486M/1.12G [00:29<00:38, 17.7MB/s]Downloading:  43%|████▎     | 488M/1.12G [00:29<00:38, 17.7MB/s]Downloading:  43%|████▎     | 489M/1.12G [00:29<00:38, 17.9MB/s]Downloading:  43%|████▎     | 491M/1.12G [00:29<00:38, 17.8MB/s]Downloading:  43%|████▎     | 493M/1.12G [00:29<00:38, 17.7MB/s]Downloading:  43%|████▎     | 494M/1.12G [00:29<00:38, 17.7MB/s]Downloading:  43%|████▎     | 496M/1.12G [00:30<00:37, 17.9MB/s]Downloading:  43%|████▎     | 498M/1.12G [00:30<00:38, 17.7MB/s]Downloading:  44%|████▎     | 500M/1.12G [00:30<00:38, 17.8MB/s]Downloading:  44%|████▍     | 501M/1.12G [00:30<00:38, 17.7MB/s]Downloading:  44%|████▍     | 503M/1.12G [00:30<00:38, 17.6MB/s]Downloading:  44%|████▍     | 505M/1.12G [00:30<00:37, 18.0MB/s]Downloading:  44%|████▍     | 507M/1.12G [00:30<00:37, 17.7MB/s]Downloading:  44%|████▍     | 509M/1.12G [00:30<00:37, 17.8MB/s]Downloading:  45%|████▍     | 510M/1.12G [00:30<00:37, 17.7MB/s]Downloading:  45%|████▍     | 512M/1.12G [00:31<00:37, 17.6MB/s]Downloading:  45%|████▍     | 514M/1.12G [00:31<00:36, 18.0MB/s]Downloading:  45%|████▌     | 516M/1.12G [00:31<00:37, 17.6MB/s]Downloading:  45%|████▌     | 518M/1.12G [00:31<00:37, 17.8MB/s]Downloading:  45%|████▌     | 519M/1.12G [00:31<00:37, 17.7MB/s]Downloading:  46%|████▌     | 521M/1.12G [00:31<00:34, 18.8MB/s]Downloading:  46%|████▌     | 523M/1.12G [00:31<00:58, 11.1MB/s]Downloading:  46%|████▌     | 525M/1.12G [00:31<00:49, 13.1MB/s]Downloading:  46%|████▌     | 527M/1.12G [00:32<00:48, 13.5MB/s]Downloading:  46%|████▌     | 529M/1.12G [00:32<00:39, 16.2MB/s]Downloading:  46%|████▋     | 531M/1.12G [00:32<00:38, 16.6MB/s]Downloading:  47%|████▋     | 533M/1.12G [00:32<00:38, 16.9MB/s]Downloading:  47%|████▋     | 534M/1.12G [00:32<00:37, 17.1MB/s]Downloading:  47%|████▋     | 536M/1.12G [00:32<00:37, 17.2MB/s]Downloading:  47%|████▋     | 538M/1.12G [00:32<00:36, 17.4MB/s]Downloading:  47%|████▋     | 540M/1.12G [00:32<00:36, 17.5MB/s]Downloading:  47%|████▋     | 541M/1.12G [00:32<00:36, 17.5MB/s]Downloading:  47%|████▋     | 543M/1.12G [00:32<00:35, 17.6MB/s]Downloading:  48%|████▊     | 545M/1.12G [00:33<00:35, 17.6MB/s]Downloading:  48%|████▊     | 546M/1.12G [00:33<00:37, 16.9MB/s]Downloading:  48%|████▊     | 548M/1.12G [00:33<00:37, 16.7MB/s]Downloading:  48%|████▊     | 550M/1.12G [00:33<00:35, 17.5MB/s]Downloading:  48%|████▊     | 552M/1.12G [00:33<00:35, 17.5MB/s]Downloading:  48%|████▊     | 553M/1.12G [00:33<00:35, 17.6MB/s]Downloading:  48%|████▊     | 555M/1.12G [00:33<00:35, 17.6MB/s]Downloading:  49%|████▊     | 557M/1.12G [00:33<00:34, 17.9MB/s]Downloading:  49%|████▉     | 558M/1.12G [00:33<00:34, 17.6MB/s]Downloading:  49%|████▉     | 560M/1.12G [00:34<00:34, 17.7MB/s]Downloading:  49%|████▉     | 562M/1.12G [00:34<00:34, 17.7MB/s]Downloading:  49%|████▉     | 564M/1.12G [00:34<00:32, 18.5MB/s]Downloading:  49%|████▉     | 566M/1.12G [00:34<00:34, 17.7MB/s]Downloading:  50%|████▉     | 567M/1.12G [00:34<00:34, 17.6MB/s]Downloading:  50%|████▉     | 569M/1.12G [00:34<00:34, 17.4MB/s]Downloading:  50%|████▉     | 571M/1.12G [00:34<00:34, 17.6MB/s]Downloading:  50%|████▉     | 572M/1.12G [00:34<00:33, 18.0MB/s]Downloading:  50%|█████     | 574M/1.12G [00:34<00:33, 17.7MB/s]Downloading:  50%|█████     | 576M/1.12G [00:34<00:33, 17.7MB/s]Downloading:  50%|█████     | 578M/1.12G [00:35<00:33, 17.7MB/s]Downloading:  51%|█████     | 580M/1.12G [00:35<00:33, 17.6MB/s]Downloading:  51%|█████     | 581M/1.12G [00:35<00:33, 17.9MB/s]Downloading:  51%|█████     | 583M/1.12G [00:35<00:33, 17.8MB/s]Downloading:  51%|█████     | 585M/1.12G [00:35<00:33, 17.7MB/s]Downloading:  51%|█████     | 586M/1.12G [00:35<00:34, 17.2MB/s]Downloading:  51%|█████▏    | 589M/1.12G [00:35<00:31, 18.8MB/s]Downloading:  52%|█████▏    | 590M/1.12G [00:35<00:32, 17.9MB/s]Downloading:  52%|█████▏    | 592M/1.12G [00:35<00:33, 17.6MB/s]Downloading:  52%|█████▏    | 594M/1.12G [00:36<00:32, 17.6MB/s]Downloading:  52%|█████▏    | 596M/1.12G [00:36<00:32, 17.6MB/s]Downloading:  52%|█████▏    | 597M/1.12G [00:36<00:32, 17.7MB/s]Downloading:  52%|█████▏    | 599M/1.12G [00:36<00:32, 17.8MB/s]Downloading:  52%|█████▏    | 601M/1.12G [00:36<00:32, 17.6MB/s]Downloading:  53%|█████▎    | 603M/1.12G [00:36<00:31, 17.8MB/s]Downloading:  53%|█████▎    | 604M/1.12G [00:36<00:32, 17.6MB/s]Downloading:  53%|█████▎    | 606M/1.12G [00:36<00:30, 18.6MB/s]Downloading:  53%|█████▎    | 608M/1.12G [00:36<00:31, 17.6MB/s]Downloading:  53%|█████▎    | 610M/1.12G [00:36<00:32, 17.4MB/s]Downloading:  53%|█████▎    | 612M/1.12G [00:37<00:32, 17.4MB/s]Downloading:  54%|█████▎    | 613M/1.12G [00:37<00:31, 17.6MB/s]Downloading:  54%|█████▎    | 615M/1.12G [00:37<00:29, 18.6MB/s]Downloading:  54%|█████▍    | 617M/1.12G [00:37<00:31, 17.8MB/s]Downloading:  54%|█████▍    | 619M/1.12G [00:37<00:31, 17.4MB/s]Downloading:  54%|█████▍    | 620M/1.12G [00:37<00:31, 17.6MB/s]Downloading:  54%|█████▍    | 622M/1.12G [00:37<00:31, 17.6MB/s]Downloading:  54%|█████▍    | 624M/1.12G [00:37<00:31, 17.6MB/s]Downloading:  55%|█████▍    | 626M/1.12G [00:37<00:30, 17.8MB/s]Downloading:  55%|█████▍    | 627M/1.12G [00:38<00:30, 17.7MB/s]Downloading:  55%|█████▍    | 629M/1.12G [00:38<00:30, 17.7MB/s]Downloading:  55%|█████▌    | 631M/1.12G [00:38<00:30, 17.7MB/s]Downloading:  55%|█████▌    | 633M/1.12G [00:38<00:28, 18.7MB/s]Downloading:  55%|█████▌    | 635M/1.12G [00:38<00:29, 17.9MB/s]Downloading:  56%|█████▌    | 637M/1.12G [00:38<00:30, 17.4MB/s]Downloading:  56%|█████▌    | 638M/1.12G [00:38<00:30, 17.5MB/s]Downloading:  56%|█████▌    | 640M/1.12G [00:38<00:29, 17.7MB/s]Downloading:  56%|█████▌    | 642M/1.12G [00:38<00:29, 17.6MB/s]Downloading:  56%|█████▌    | 644M/1.12G [00:38<00:29, 17.8MB/s]Downloading:  56%|█████▋    | 645M/1.12G [00:39<00:29, 17.6MB/s]Downloading:  57%|█████▋    | 647M/1.12G [00:39<00:29, 17.7MB/s]Downloading:  57%|█████▋    | 649M/1.12G [00:39<00:29, 17.8MB/s]Downloading:  57%|█████▋    | 651M/1.12G [00:39<00:27, 18.6MB/s]Downloading:  57%|█████▋    | 653M/1.12G [00:39<00:29, 17.7MB/s]Downloading:  57%|█████▋    | 654M/1.12G [00:39<00:29, 17.5MB/s]Downloading:  57%|█████▋    | 656M/1.12G [00:39<00:29, 17.5MB/s]Downloading:  57%|█████▋    | 658M/1.12G [00:39<00:28, 17.7MB/s]Downloading:  58%|█████▊    | 660M/1.12G [00:39<00:28, 17.7MB/s]Downloading:  58%|█████▊    | 662M/1.12G [00:40<00:28, 18.0MB/s]Downloading:  58%|█████▊    | 663M/1.12G [00:40<00:28, 17.8MB/s]Downloading:  58%|█████▊    | 665M/1.12G [00:40<00:28, 17.7MB/s]Downloading:  58%|█████▊    | 667M/1.12G [00:40<00:28, 17.7MB/s]Downloading:  58%|█████▊    | 669M/1.12G [00:40<00:28, 17.8MB/s]Downloading:  59%|█████▊    | 670M/1.12G [00:40<00:27, 17.9MB/s]Downloading:  59%|█████▊    | 672M/1.12G [00:40<00:27, 17.7MB/s]Downloading:  59%|█████▉    | 674M/1.12G [00:40<00:27, 17.7MB/s]Downloading:  59%|█████▉    | 676M/1.12G [00:40<00:27, 17.6MB/s]Downloading:  59%|█████▉    | 678M/1.12G [00:40<00:26, 18.7MB/s]Downloading:  59%|█████▉    | 679M/1.12G [00:41<00:27, 17.7MB/s]Downloading:  59%|█████▉    | 681M/1.12G [00:41<00:27, 17.5MB/s]Downloading:  60%|█████▉    | 683M/1.12G [00:41<00:27, 17.5MB/s]Downloading:  60%|█████▉    | 685M/1.12G [00:41<00:27, 17.6MB/s]Downloading:  60%|█████▉    | 686M/1.12G [00:41<00:27, 17.5MB/s]Downloading:  60%|██████    | 688M/1.12G [00:41<00:26, 17.7MB/s]Downloading:  60%|██████    | 690M/1.12G [00:41<00:27, 17.5MB/s]Downloading:  60%|██████    | 692M/1.12G [00:41<00:28, 16.7MB/s]Downloading:  61%|██████    | 693M/1.12G [00:42<00:41, 11.4MB/s]Downloading:  61%|██████    | 695M/1.12G [00:42<00:39, 12.1MB/s]Downloading:  61%|██████    | 696M/1.12G [00:42<00:34, 13.6MB/s]Downloading:  61%|██████    | 699M/1.12G [00:42<00:28, 16.4MB/s]Downloading:  61%|██████    | 701M/1.12G [00:42<00:27, 16.7MB/s]Downloading:  61%|██████▏   | 702M/1.12G [00:42<00:27, 16.9MB/s]Downloading:  61%|██████▏   | 704M/1.12G [00:42<00:27, 17.0MB/s]Downloading:  62%|██████▏   | 706M/1.12G [00:42<00:26, 17.1MB/s]Downloading:  62%|██████▏   | 707M/1.12G [00:42<00:26, 17.1MB/s]Downloading:  62%|██████▏   | 709M/1.12G [00:43<00:26, 17.3MB/s]Downloading:  62%|██████▏   | 711M/1.12G [00:43<00:26, 17.3MB/s]Downloading:  62%|██████▏   | 712M/1.12G [00:43<00:26, 17.3MB/s]Downloading:  62%|██████▏   | 714M/1.12G [00:43<00:26, 16.9MB/s]Downloading:  62%|██████▏   | 716M/1.12G [00:43<00:26, 17.2MB/s]Downloading:  63%|██████▎   | 717M/1.12G [00:43<00:26, 17.0MB/s]Downloading:  63%|██████▎   | 719M/1.12G [00:43<00:24, 18.1MB/s]Downloading:  63%|██████▎   | 721M/1.12G [00:43<00:24, 17.9MB/s]Downloading:  63%|██████▎   | 723M/1.12G [00:43<00:25, 17.7MB/s]Downloading:  63%|██████▎   | 724M/1.12G [00:43<00:25, 17.6MB/s]Downloading:  63%|██████▎   | 726M/1.12G [00:44<00:25, 17.6MB/s]Downloading:  64%|██████▎   | 728M/1.12G [00:44<00:24, 17.5MB/s]Downloading:  64%|██████▎   | 730M/1.12G [00:44<00:24, 17.5MB/s]Downloading:  64%|██████▍   | 731M/1.12G [00:44<00:24, 17.4MB/s]Downloading:  64%|██████▍   | 733M/1.12G [00:44<00:24, 17.6MB/s]Downloading:  64%|██████▍   | 735M/1.12G [00:44<00:24, 17.2MB/s]Downloading:  64%|██████▍   | 736M/1.12G [00:44<00:25, 16.6MB/s]Downloading:  64%|██████▍   | 738M/1.12G [00:44<00:23, 18.5MB/s]Downloading:  65%|██████▍   | 740M/1.12G [00:44<00:23, 18.2MB/s]Downloading:  65%|██████▍   | 742M/1.12G [00:44<00:23, 18.0MB/s]Downloading:  65%|██████▍   | 744M/1.12G [00:45<00:23, 17.8MB/s]Downloading:  65%|██████▌   | 745M/1.12G [00:45<00:23, 17.7MB/s]Downloading:  65%|██████▌   | 747M/1.12G [00:45<00:23, 17.6MB/s]Downloading:  65%|██████▌   | 749M/1.12G [00:45<00:23, 17.6MB/s]Downloading:  66%|██████▌   | 750M/1.12G [00:45<00:23, 17.5MB/s]Downloading:  66%|██████▌   | 752M/1.12G [00:45<00:23, 17.5MB/s]Downloading:  66%|██████▌   | 754M/1.12G [00:45<00:24, 16.9MB/s]Downloading:  66%|██████▌   | 756M/1.12G [00:45<00:23, 17.3MB/s]Downloading:  66%|██████▌   | 757M/1.12G [00:45<00:22, 18.1MB/s]Downloading:  66%|██████▋   | 759M/1.12G [00:45<00:22, 17.9MB/s]Downloading:  66%|██████▋   | 761M/1.12G [00:46<00:22, 17.7MB/s]Downloading:  67%|██████▋   | 763M/1.12G [00:46<00:22, 17.6MB/s]Downloading:  67%|██████▋   | 764M/1.12G [00:46<00:22, 17.6MB/s]Downloading:  67%|██████▋   | 766M/1.12G [00:46<00:22, 17.5MB/s]Downloading:  67%|██████▋   | 768M/1.12G [00:46<00:22, 17.5MB/s]Downloading:  67%|██████▋   | 769M/1.12G [00:46<00:22, 17.4MB/s]Downloading:  67%|██████▋   | 771M/1.12G [00:46<00:22, 17.4MB/s]Downloading:  67%|██████▋   | 773M/1.12G [00:46<00:22, 17.4MB/s]Downloading:  68%|██████▊   | 774M/1.12G [00:46<00:23, 16.9MB/s]Downloading:  68%|██████▊   | 776M/1.12G [00:47<00:22, 17.4MB/s]Downloading:  68%|██████▊   | 778M/1.12G [00:47<00:21, 18.1MB/s]Downloading:  68%|██████▊   | 780M/1.12G [00:47<00:21, 17.9MB/s]Downloading:  68%|██████▊   | 781M/1.12G [00:47<00:21, 17.7MB/s]Downloading:  68%|██████▊   | 783M/1.12G [00:47<00:21, 17.6MB/s]Downloading:  69%|██████▊   | 785M/1.12G [00:47<00:21, 17.4MB/s]Downloading:  69%|██████▊   | 786M/1.12G [00:47<00:21, 17.5MB/s]Downloading:  69%|██████▉   | 788M/1.12G [00:47<00:21, 17.5MB/s]Downloading:  69%|██████▉   | 790M/1.12G [00:47<00:21, 17.4MB/s]Downloading:  69%|██████▉   | 791M/1.12G [00:47<00:21, 17.4MB/s]Downloading:  69%|██████▉   | 793M/1.12G [00:48<00:21, 17.5MB/s]Downloading:  69%|██████▉   | 795M/1.12G [00:48<00:22, 16.5MB/s]Downloading:  70%|██████▉   | 797M/1.12G [00:48<00:20, 17.7MB/s]Downloading:  70%|██████▉   | 799M/1.12G [00:48<00:19, 18.3MB/s]Downloading:  70%|██████▉   | 800M/1.12G [00:48<00:20, 18.1MB/s]Downloading:  70%|███████   | 802M/1.12G [00:48<00:20, 17.8MB/s]Downloading:  70%|███████   | 804M/1.12G [00:48<00:20, 17.8MB/s]Downloading:  70%|███████   | 806M/1.12G [00:48<00:20, 17.6MB/s]Downloading:  70%|███████   | 807M/1.12G [00:48<00:20, 17.6MB/s]Downloading:  71%|███████   | 809M/1.12G [00:48<00:20, 17.6MB/s]Downloading:  71%|███████   | 811M/1.12G [00:49<00:20, 17.5MB/s]Downloading:  71%|███████   | 812M/1.12G [00:49<00:19, 17.5MB/s]Downloading:  71%|███████   | 814M/1.12G [00:49<00:19, 17.5MB/s]Downloading:  71%|███████   | 816M/1.12G [00:49<00:20, 16.5MB/s]Downloading:  71%|███████▏  | 818M/1.12G [00:49<00:19, 17.4MB/s]Downloading:  72%|███████▏  | 819M/1.12G [00:49<00:18, 18.2MB/s]Downloading:  72%|███████▏  | 821M/1.12G [00:49<00:19, 17.9MB/s]Downloading:  72%|███████▏  | 823M/1.12G [00:49<00:19, 17.6MB/s]Downloading:  72%|███████▏  | 825M/1.12G [00:49<00:19, 17.5MB/s]Downloading:  72%|███████▏  | 826M/1.12G [00:50<00:19, 17.4MB/s]Downloading:  72%|███████▏  | 828M/1.12G [00:50<00:19, 17.3MB/s]Downloading:  72%|███████▏  | 830M/1.12G [00:50<00:19, 17.3MB/s]Downloading:  73%|███████▎  | 831M/1.12G [00:50<00:19, 17.3MB/s]Downloading:  73%|███████▎  | 833M/1.12G [00:50<00:19, 17.2MB/s]Downloading:  73%|███████▎  | 835M/1.12G [00:50<00:18, 17.3MB/s]Downloading:  73%|███████▎  | 837M/1.12G [00:50<00:18, 17.8MB/s]Downloading:  73%|███████▎  | 838M/1.12G [00:50<00:18, 17.8MB/s]Downloading:  73%|███████▎  | 840M/1.12G [00:50<00:17, 18.6MB/s]Downloading:  74%|███████▎  | 842M/1.12G [00:50<00:17, 18.2MB/s]Downloading:  74%|███████▎  | 844M/1.12G [00:51<00:17, 17.9MB/s]Downloading:  74%|███████▍  | 846M/1.12G [00:51<00:17, 17.7MB/s]Downloading:  74%|███████▍  | 847M/1.12G [00:51<00:17, 17.6MB/s]Downloading:  74%|███████▍  | 849M/1.12G [00:51<00:17, 17.4MB/s]Downloading:  74%|███████▍  | 851M/1.12G [00:51<00:17, 17.4MB/s]Downloading:  74%|███████▍  | 852M/1.12G [00:51<00:17, 17.3MB/s]Downloading:  75%|███████▍  | 854M/1.12G [00:51<00:17, 17.3MB/s]Downloading:  75%|███████▍  | 856M/1.12G [00:51<00:17, 17.3MB/s]Downloading:  75%|███████▍  | 857M/1.12G [00:51<00:18, 16.7MB/s]Downloading:  75%|███████▌  | 859M/1.12G [00:51<00:18, 16.4MB/s]Downloading:  75%|███████▌  | 861M/1.12G [00:52<00:18, 15.8MB/s]Downloading:  75%|███████▌  | 862M/1.12G [00:52<00:26, 11.2MB/s]Downloading:  75%|███████▌  | 864M/1.12G [00:52<00:23, 12.6MB/s]Downloading:  76%|███████▌  | 866M/1.12G [00:52<00:21, 13.7MB/s]Downloading:  76%|███████▌  | 867M/1.12G [00:52<00:19, 14.6MB/s]Downloading:  76%|███████▌  | 869M/1.12G [00:52<00:18, 15.3MB/s]Downloading:  76%|███████▌  | 871M/1.12G [00:52<00:18, 15.8MB/s]Downloading:  76%|███████▌  | 872M/1.12G [00:52<00:17, 16.3MB/s]Downloading:  76%|███████▋  | 874M/1.12G [00:53<00:17, 16.5MB/s]Downloading:  76%|███████▋  | 875M/1.12G [00:53<00:16, 16.8MB/s]Downloading:  77%|███████▋  | 877M/1.12G [00:53<00:16, 17.1MB/s]Downloading:  77%|███████▋  | 879M/1.12G [00:53<00:15, 17.5MB/s]Downloading:  77%|███████▋  | 881M/1.12G [00:53<00:15, 17.6MB/s]Downloading:  77%|███████▋  | 883M/1.12G [00:53<00:14, 19.5MB/s]Downloading:  77%|███████▋  | 885M/1.12G [00:53<00:14, 18.9MB/s]Downloading:  77%|███████▋  | 887M/1.12G [00:53<00:15, 18.0MB/s]Downloading:  78%|███████▊  | 889M/1.12G [00:53<00:14, 18.3MB/s]Downloading:  78%|███████▊  | 891M/1.12G [00:54<00:14, 18.0MB/s]Downloading:  78%|███████▊  | 892M/1.12G [00:54<00:14, 17.9MB/s]Downloading:  78%|███████▊  | 894M/1.12G [00:54<00:14, 17.8MB/s]Downloading:  78%|███████▊  | 896M/1.12G [00:54<00:14, 17.8MB/s]Downloading:  78%|███████▊  | 897M/1.12G [00:54<00:14, 17.7MB/s]Downloading:  79%|███████▊  | 899M/1.12G [00:54<00:14, 17.6MB/s]Downloading:  79%|███████▊  | 901M/1.12G [00:54<00:14, 17.7MB/s]Downloading:  79%|███████▉  | 902M/1.12G [00:54<00:14, 17.1MB/s]Downloading:  79%|███████▉  | 904M/1.12G [00:54<00:14, 17.1MB/s]Downloading:  79%|███████▉  | 906M/1.12G [00:54<00:13, 18.1MB/s]Downloading:  79%|███████▉  | 908M/1.12G [00:55<00:14, 17.3MB/s]Downloading:  79%|███████▉  | 909M/1.12G [00:55<00:14, 17.3MB/s]Downloading:  80%|███████▉  | 911M/1.12G [00:55<00:14, 17.3MB/s]Downloading:  80%|███████▉  | 913M/1.12G [00:55<00:13, 17.4MB/s]Downloading:  80%|███████▉  | 915M/1.12G [00:55<00:12, 18.6MB/s]Downloading:  80%|████████  | 917M/1.12G [00:55<00:13, 17.6MB/s]Downloading:  80%|████████  | 919M/1.12G [00:55<00:13, 17.5MB/s]Downloading:  80%|████████  | 920M/1.12G [00:55<00:13, 17.5MB/s]Downloading:  81%|████████  | 922M/1.12G [00:55<00:13, 17.5MB/s]Downloading:  81%|████████  | 924M/1.12G [00:56<00:13, 17.5MB/s]Downloading:  81%|████████  | 926M/1.12G [00:56<00:12, 17.7MB/s]Downloading:  81%|████████  | 927M/1.12G [00:56<00:12, 17.6MB/s]Downloading:  81%|████████  | 929M/1.12G [00:56<00:12, 18.4MB/s]Downloading:  81%|████████▏ | 931M/1.12G [00:56<00:12, 17.5MB/s]Downloading:  81%|████████▏ | 933M/1.12G [00:56<00:12, 18.5MB/s]Downloading:  82%|████████▏ | 935M/1.12G [00:56<00:12, 17.6MB/s]Downloading:  82%|████████▏ | 937M/1.12G [00:56<00:12, 17.4MB/s]Downloading:  82%|████████▏ | 938M/1.12G [00:56<00:12, 17.5MB/s]Downloading:  82%|████████▏ | 940M/1.12G [00:56<00:12, 17.5MB/s]Downloading:  82%|████████▏ | 942M/1.12G [00:57<00:12, 17.5MB/s]Downloading:  82%|████████▏ | 944M/1.12G [00:57<00:11, 17.9MB/s]Downloading:  83%|████████▎ | 945M/1.12G [00:57<00:11, 17.6MB/s]Downloading:  83%|████████▎ | 947M/1.12G [00:57<00:11, 17.5MB/s]Downloading:  83%|████████▎ | 949M/1.12G [00:57<00:11, 17.6MB/s]Downloading:  83%|████████▎ | 951M/1.12G [00:57<00:11, 17.7MB/s]Downloading:  83%|████████▎ | 953M/1.12G [00:57<00:11, 17.8MB/s]Downloading:  83%|████████▎ | 954M/1.12G [00:57<00:11, 17.7MB/s]Downloading:  83%|████████▎ | 956M/1.12G [00:57<00:11, 17.1MB/s]Downloading:  84%|████████▎ | 958M/1.12G [00:58<00:11, 17.1MB/s]Downloading:  84%|████████▍ | 960M/1.12G [00:58<00:11, 17.3MB/s]Downloading:  84%|████████▍ | 961M/1.12G [00:58<00:10, 17.8MB/s]Downloading:  84%|████████▍ | 963M/1.12G [00:58<00:11, 17.1MB/s]Downloading:  84%|████████▍ | 965M/1.12G [00:58<00:11, 17.0MB/s]Downloading:  84%|████████▍ | 966M/1.12G [00:58<00:11, 16.8MB/s]Downloading:  85%|████████▍ | 968M/1.12G [00:58<00:11, 16.5MB/s]Downloading:  85%|████████▍ | 970M/1.12G [00:58<00:10, 17.6MB/s]Downloading:  85%|████████▍ | 972M/1.12G [00:58<00:11, 16.5MB/s]Downloading:  85%|████████▍ | 973M/1.12G [00:58<00:11, 16.4MB/s]Downloading:  85%|████████▌ | 975M/1.12G [00:59<00:10, 16.6MB/s]Downloading:  85%|████████▌ | 977M/1.12G [00:59<00:09, 17.8MB/s]Downloading:  85%|████████▌ | 979M/1.12G [00:59<00:10, 16.9MB/s]Downloading:  86%|████████▌ | 980M/1.12G [00:59<00:10, 17.2MB/s]Downloading:  86%|████████▌ | 982M/1.12G [00:59<00:09, 17.2MB/s]Downloading:  86%|████████▌ | 984M/1.12G [00:59<00:09, 17.5MB/s]Downloading:  86%|████████▌ | 985M/1.12G [00:59<00:09, 17.5MB/s]Downloading:  86%|████████▌ | 987M/1.12G [00:59<00:09, 18.1MB/s]Downloading:  86%|████████▋ | 989M/1.12G [00:59<00:09, 17.5MB/s]Downloading:  87%|████████▋ | 991M/1.12G [01:00<00:09, 17.4MB/s]Downloading:  87%|████████▋ | 992M/1.12G [01:00<00:09, 17.5MB/s]Downloading:  87%|████████▋ | 994M/1.12G [01:00<00:09, 17.6MB/s]Downloading:  87%|████████▋ | 996M/1.12G [01:00<00:08, 18.1MB/s]Downloading:  87%|████████▋ | 998M/1.12G [01:00<00:08, 17.6MB/s]Downloading:  87%|████████▋ | 999M/1.12G [01:00<00:08, 17.8MB/s]Downloading:  87%|████████▋ | 0.98G/1.12G [01:00<00:08, 17.7MB/s]Downloading:  88%|████████▊ | 0.98G/1.12G [01:00<00:08, 17.5MB/s]Downloading:  88%|████████▊ | 0.98G/1.12G [01:00<00:08, 17.9MB/s]Downloading:  88%|████████▊ | 0.98G/1.12G [01:00<00:08, 17.6MB/s]Downloading:  88%|████████▊ | 0.98G/1.12G [01:01<00:07, 18.0MB/s]Downloading:  88%|████████▊ | 0.99G/1.12G [01:01<00:08, 17.7MB/s]Downloading:  88%|████████▊ | 0.99G/1.12G [01:01<00:07, 18.6MB/s]Downloading:  89%|████████▊ | 0.99G/1.12G [01:01<00:07, 17.6MB/s]Downloading:  89%|████████▊ | 0.99G/1.12G [01:01<00:07, 17.5MB/s]Downloading:  89%|████████▉ | 0.99G/1.12G [01:01<00:07, 17.6MB/s]Downloading:  89%|████████▉ | 1.00G/1.12G [01:01<00:07, 17.6MB/s]Downloading:  89%|████████▉ | 1.00G/1.12G [01:01<00:07, 17.6MB/s]Downloading:  89%|████████▉ | 1.00G/1.12G [01:01<00:07, 17.7MB/s]Downloading:  89%|████████▉ | 1.00G/1.12G [01:02<00:07, 17.6MB/s]Downloading:  90%|████████▉ | 1.00G/1.12G [01:02<00:07, 17.6MB/s]Downloading:  90%|████████▉ | 1.00G/1.12G [01:02<00:06, 17.6MB/s]Downloading:  90%|████████▉ | 1.01G/1.12G [01:02<00:10, 11.1MB/s]Downloading:  90%|█████████ | 1.01G/1.12G [01:02<00:09, 13.0MB/s]Downloading:  90%|█████████ | 1.01G/1.12G [01:02<00:08, 13.5MB/s]Downloading:  90%|█████████ | 1.01G/1.12G [01:02<00:07, 14.5MB/s]Downloading:  91%|█████████ | 1.01G/1.12G [01:02<00:06, 17.2MB/s]Downloading:  91%|█████████ | 1.01G/1.12G [01:03<00:06, 17.3MB/s]Downloading:  91%|█████████ | 1.02G/1.12G [01:03<00:06, 17.3MB/s]Downloading:  91%|█████████ | 1.02G/1.12G [01:03<00:06, 17.3MB/s]Downloading:  91%|█████████ | 1.02G/1.12G [01:03<00:06, 17.4MB/s]Downloading:  91%|█████████▏| 1.02G/1.12G [01:03<00:05, 17.4MB/s]Downloading:  91%|█████████▏| 1.02G/1.12G [01:03<00:05, 17.4MB/s]Downloading:  92%|█████████▏| 1.02G/1.12G [01:03<00:05, 17.4MB/s]Downloading:  92%|█████████▏| 1.03G/1.12G [01:03<00:05, 17.4MB/s]Downloading:  92%|█████████▏| 1.03G/1.12G [01:03<00:05, 17.5MB/s]Downloading:  92%|█████████▏| 1.03G/1.12G [01:03<00:05, 17.4MB/s]Downloading:  92%|█████████▏| 1.03G/1.12G [01:04<00:05, 17.4MB/s]Downloading:  92%|█████████▏| 1.03G/1.12G [01:04<00:05, 18.2MB/s]Downloading:  93%|█████████▎| 1.03G/1.12G [01:04<00:04, 18.0MB/s]Downloading:  93%|█████████▎| 1.04G/1.12G [01:04<00:04, 17.9MB/s]Downloading:  93%|█████████▎| 1.04G/1.12G [01:04<00:04, 17.8MB/s]Downloading:  93%|█████████▎| 1.04G/1.12G [01:04<00:04, 17.8MB/s]Downloading:  93%|█████████▎| 1.04G/1.12G [01:04<00:04, 17.7MB/s]Downloading:  93%|█████████▎| 1.04G/1.12G [01:04<00:04, 17.6MB/s]Downloading:  93%|█████████▎| 1.04G/1.12G [01:04<00:04, 17.6MB/s]Downloading:  94%|█████████▎| 1.05G/1.12G [01:04<00:04, 17.6MB/s]Downloading:  94%|█████████▎| 1.05G/1.12G [01:05<00:04, 17.6MB/s]Downloading:  94%|█████████▍| 1.05G/1.12G [01:05<00:04, 17.6MB/s]Downloading:  94%|█████████▍| 1.05G/1.12G [01:05<00:04, 17.2MB/s]Downloading:  94%|█████████▍| 1.05G/1.12G [01:05<00:04, 17.5MB/s]Downloading:  94%|█████████▍| 1.05G/1.12G [01:05<00:03, 18.4MB/s]Downloading:  94%|█████████▍| 1.06G/1.12G [01:05<00:03, 18.1MB/s]Downloading:  95%|█████████▍| 1.06G/1.12G [01:05<00:03, 18.0MB/s]Downloading:  95%|█████████▍| 1.06G/1.12G [01:05<00:03, 17.9MB/s]Downloading:  95%|█████████▍| 1.06G/1.12G [01:05<00:03, 17.9MB/s]Downloading:  95%|█████████▌| 1.06G/1.12G [01:06<00:03, 17.8MB/s]Downloading:  95%|█████████▌| 1.06G/1.12G [01:06<00:03, 17.7MB/s]Downloading:  95%|█████████▌| 1.07G/1.12G [01:06<00:03, 17.7MB/s]Downloading:  96%|█████████▌| 1.07G/1.12G [01:06<00:03, 17.7MB/s]Downloading:  96%|█████████▌| 1.07G/1.12G [01:06<00:02, 17.7MB/s]Downloading:  96%|█████████▌| 1.07G/1.12G [01:06<00:02, 17.4MB/s]Downloading:  96%|█████████▌| 1.07G/1.12G [01:06<00:02, 17.3MB/s]Downloading:  96%|█████████▌| 1.08G/1.12G [01:06<00:02, 18.2MB/s]Downloading:  96%|█████████▋| 1.08G/1.12G [01:06<00:02, 18.0MB/s]Downloading:  96%|█████████▋| 1.08G/1.12G [01:06<00:02, 17.9MB/s]Downloading:  97%|█████████▋| 1.08G/1.12G [01:07<00:02, 17.8MB/s]Downloading:  97%|█████████▋| 1.08G/1.12G [01:07<00:02, 17.7MB/s]Downloading:  97%|█████████▋| 1.08G/1.12G [01:07<00:02, 17.7MB/s]Downloading:  97%|█████████▋| 1.09G/1.12G [01:07<00:02, 17.6MB/s]Downloading:  97%|█████████▋| 1.09G/1.12G [01:07<00:01, 17.6MB/s]Downloading:  97%|█████████▋| 1.09G/1.12G [01:07<00:01, 17.6MB/s]Downloading:  97%|█████████▋| 1.09G/1.12G [01:07<00:01, 17.6MB/s]Downloading:  98%|█████████▊| 1.09G/1.12G [01:07<00:01, 17.3MB/s]Downloading:  98%|█████████▊| 1.09G/1.12G [01:07<00:01, 17.5MB/s]Downloading:  98%|█████████▊| 1.10G/1.12G [01:07<00:01, 18.3MB/s]Downloading:  98%|█████████▊| 1.10G/1.12G [01:08<00:01, 18.1MB/s]Downloading:  98%|█████████▊| 1.10G/1.12G [01:08<00:01, 18.0MB/s]Downloading:  98%|█████████▊| 1.10G/1.12G [01:08<00:01, 17.8MB/s]Downloading:  99%|█████████▊| 1.10G/1.12G [01:08<00:00, 17.8MB/s]Downloading:  99%|█████████▊| 1.10G/1.12G [01:08<00:00, 17.7MB/s]Downloading:  99%|█████████▉| 1.11G/1.12G [01:08<00:00, 17.6MB/s]Downloading:  99%|█████████▉| 1.11G/1.12G [01:08<00:00, 17.6MB/s]Downloading:  99%|█████████▉| 1.11G/1.12G [01:08<00:00, 17.6MB/s]Downloading:  99%|█████████▉| 1.11G/1.12G [01:08<00:00, 17.6MB/s]Downloading:  99%|█████████▉| 1.11G/1.12G [01:08<00:00, 17.8MB/s]Downloading: 100%|█████████▉| 1.11G/1.12G [01:09<00:00, 17.4MB/s]Downloading: 100%|█████████▉| 1.12G/1.12G [01:09<00:00, 17.1MB/s]Downloading: 100%|█████████▉| 1.12G/1.12G [01:09<00:00, 18.3MB/s]Downloading: 100%|██████████| 1.12G/1.12G [01:09<00:00, 17.3MB/s]
2022-10-07 12:35:27.725 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 174: Load path /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/ms_marco_base is given.
2022-10-07 12:35:27.726 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 203: Init from scratch. Load path /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/ms_marco_base.pth.tar does not exist.
2022-10-07 12:35:27.728 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 102: Model was successfully initialized! Model summary:
 MT5ForConditionalGeneration(
  (shared): Embedding(250112, 512)
  (encoder): T5Stack(
    (embed_tokens): Embedding(250112, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 6)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(250112, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 6)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=512, out_features=250112, bias=False)
)
/data/home/admin/bachelor/env/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2227: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  FutureWarning,
2022-10-07 12:36:24.324 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 199: Initial best ppl of 1.1738463503150296e+20
2022-10-07 12:45:24.444 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.1738463503150296e+20 to 2.4598
2022-10-07 12:45:24.444 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 12:45:24.444 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 12:54:24.680 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 2.4598 to 1.9459
2022-10-07 12:54:24.681 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 12:54:24.681 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 13:03:14.55 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.9459 to 1.8376
2022-10-07 13:03:14.55 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 13:03:14.56 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 13:12:01.562 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.8376 to 1.7693
2022-10-07 13:12:01.562 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 13:12:01.562 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 13:20:51.194 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.7693 to 1.6903
2022-10-07 13:20:51.194 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 13:20:51.195 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 13:29:43.218 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.6903 to 1.6242
2022-10-07 13:29:43.218 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 13:29:43.218 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 13:38:20.633 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.6242 to 1.5934
2022-10-07 13:38:20.633 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 13:38:20.633 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 13:46:57.734 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.5934 to 1.5749
2022-10-07 13:46:57.735 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 13:46:57.735 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 13:55:38.169 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.5749 to 1.5688
2022-10-07 13:55:38.169 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 13:55:38.169 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 14:04:03.211 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.5688 to 1.5529
2022-10-07 14:04:03.212 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 14:04:03.212 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 14:12:28.834 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.5529 to 1.4966
2022-10-07 14:12:28.834 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 14:12:28.835 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 14:20:37.742 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.4966
2022-10-07 14:28:29.84 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4966 to 1.4897
2022-10-07 14:28:29.84 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 14:28:29.84 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 14:36:31.746 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4897 to 1.4812
2022-10-07 14:36:31.746 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 14:36:31.746 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 14:44:56.826 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4812 to 1.4673
2022-10-07 14:44:56.826 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 14:44:56.826 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 14:53:10.465 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4673 to 1.4584
2022-10-07 14:53:10.465 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 14:53:10.465 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 15:01:08.144 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.4584
2022-10-07 15:08:54.442 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.4584
2022-10-07 15:16:37.12 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4584 to 1.4493
2022-10-07 15:16:37.12 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 15:16:37.12 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 15:24:29.224 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4493 to 1.4374
2022-10-07 15:24:29.226 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 15:24:29.226 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 15:32:31.295 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4374 to 1.4229
2022-10-07 15:32:31.295 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 15:32:31.296 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 15:40:32.640 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4229 to 1.4164
2022-10-07 15:40:32.640 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 15:40:32.641 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 15:48:20.988 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4164 to 1.4127
2022-10-07 15:48:20.988 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 15:48:20.988 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 15:56:12.131 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.4127
2022-10-07 16:03:56.627 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4127 to 1.4083
2022-10-07 16:03:56.627 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 16:03:56.627 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 16:11:46.267 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.4083
2022-10-07 16:19:26.55 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4083 to 1.4049
2022-10-07 16:19:26.55 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 16:19:26.56 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 16:27:22.285 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.4049
2022-10-07 16:35:02.223 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.4049 to 1.3905
2022-10-07 16:35:02.223 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 16:35:02.224 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 16:43:02.885 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3905
2022-10-07 16:50:34.722 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3905
2022-10-07 16:57:54.759 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3905
2022-10-07 17:05:41.504 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3905
2022-10-07 17:13:13.249 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3905
2022-10-07 17:20:47.64 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.3905 to 1.3904
2022-10-07 17:20:47.64 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 17:20:47.64 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 17:28:40.424 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3904
2022-10-07 17:36:15.455 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.3904 to 1.3867
2022-10-07 17:36:15.455 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 17:36:15.455 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 17:43:54.838 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.3867 to 1.383
2022-10-07 17:43:54.839 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 17:43:54.839 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 17:51:47.245 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.383
2022-10-07 17:59:29.175 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.383
2022-10-07 18:07:08.948 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.383
2022-10-07 18:14:43.965 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.383 to 1.368
2022-10-07 18:14:43.965 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 18:14:43.965 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 18:22:23.169 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.368 to 1.3656
2022-10-07 18:22:23.169 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 18:22:23.170 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 18:30:00.334 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3656
2022-10-07 18:37:24.158 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3656
2022-10-07 18:44:54.602 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3656
2022-10-07 18:52:09.562 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3656
2022-10-07 18:59:41.228 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.3656 to 1.3641
2022-10-07 18:59:41.228 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 18:59:41.228 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 19:07:34.851 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3641
2022-10-07 19:15:06.492 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.3641 to 1.3627
2022-10-07 19:15:06.492 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 19:15:06.492 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 19:22:51.55 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3627
2022-10-07 19:30:21.86 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.3627 to 1.3563
2022-10-07 19:30:21.87 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 19:30:21.87 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 19:38:14.295 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 212: Did not improve on the ppl of 1.3563
2022-10-07 19:45:38.119 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 207: Improved best ppl from 1.3563 to 1.3561
2022-10-07 19:45:38.119 INFO in 'deeppavlov.core.trainers.nn_trainer'['nn_trainer'] at line 209: Saving model
2022-10-07 19:45:38.119 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 195: Saving model to /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 19:49:28.695 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 45: Load path '/data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/ms_marco_base' differs from save path '/data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model' in 'infer' mode for TorchGenerativeQA.
2022-10-07 19:49:28.695 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 145: From pretrained google/mt5-small.
2022-10-07 19:49:37.531 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 174: Load path /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/ms_marco_base is given.
2022-10-07 19:49:37.532 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 203: Init from scratch. Load path /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/ms_marco_base.pth.tar does not exist.
2022-10-07 19:49:37.535 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 102: Model was successfully initialized! Model summary:
 MT5ForConditionalGeneration(
  (shared): Embedding(250112, 512)
  (encoder): T5Stack(
    (embed_tokens): Embedding(250112, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 6)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(250112, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 6)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=512, out_features=250112, bias=False)
)
2022-10-07 19:50:52.423 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 145: From pretrained google/mt5-small.
2022-10-07 19:50:58.532 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 174: Load path /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model is given.
2022-10-07 19:50:58.533 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 181: Load path /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar exists.
2022-10-07 19:50:58.533 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 182: Initializing `TorchGenerativeQA` from saved.
2022-10-07 19:50:58.533 INFO in 'deeppavlov.models.torch_bert.torch_generative_qa'['torch_generative_qa'] at line 185: Loading weights from /data/home/admin/.deeppavlov/models/generative_qa/google/mt5-small-ms-marco/new_model.pth.tar.
2022-10-07 19:51:00.975 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 102: Model was successfully initialized! Model summary:
 MT5ForConditionalGeneration(
  (shared): Embedding(250112, 512)
  (encoder): T5Stack(
    (embed_tokens): Embedding(250112, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 6)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(250112, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 6)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (6): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (7): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=384, bias=False)
              (k): Linear(in_features=512, out_features=384, bias=False)
              (v): Linear(in_features=512, out_features=384, bias=False)
              (o): Linear(in_features=384, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseGatedGeluDense(
              (wi_0): Linear(in_features=512, out_features=1024, bias=False)
              (wi_1): Linear(in_features=512, out_features=1024, bias=False)
              (wo): Linear(in_features=1024, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=512, out_features=250112, bias=False)
)
